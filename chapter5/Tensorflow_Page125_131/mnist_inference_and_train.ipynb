{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 定义神经网络结构的相关参数\n",
    "INPUT_NODE = 784\n",
    "OUTPUT_NODE = 10\n",
    "LAYER1_NODE = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_weight_variable(shape, regularizer):\n",
    "    '''通过tf.get_variable函数来获取变量'''\n",
    "    #print(\"name:\",tf.get_variable_scope().name)\n",
    "    weights = tf.get_variable(\"weights\", shape, initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    \n",
    "    # 当给出了正则化生成函数时，将当前变量的正则化损失加入名字为losses的集合\n",
    "    if regularizer != None:\n",
    "        tf.add_to_collection('losses', regularizer(weights))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(input_tensor, regularizer):\n",
    "    '''定义神经网络的前向传播过程'''\n",
    "    # 声明第一层神经网络的变量并完成前向传播过程\n",
    "    with tf.variable_scope('layer1'):\n",
    "        weights = get_weight_variable([INPUT_NODE,LAYER1_NODE],regularizer)\n",
    "        biases = tf.get_variable(\"biases\", [LAYER1_NODE], initializer=tf.constant_initializer(0.0))\n",
    "        layer1 = tf.nn.relu(tf.matmul(input_tensor, weights) + biases)\n",
    "    \n",
    "    # 声明第二层\n",
    "    with tf.variable_scope('layer2'):\n",
    "        weights = get_weight_variable([LAYER1_NODE,OUTPUT_NODE],regularizer)\n",
    "        biases = tf.get_variable(\"biases\", [OUTPUT_NODE], initializer=tf.constant_initializer(0.0))\n",
    "        layer2 = tf.matmul(layer1, weights) + biases\n",
    "    \n",
    "    for ele1 in tf.trainable_variables():  \n",
    "        print(ele1.name)\n",
    "    return layer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "\n",
    "# 配置神经网络中的参数\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE_BASE = 0.8\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "REGULARAZTION_RATE = 0.0001\n",
    "TRAINING_STEPS = 30000\n",
    "MOVING_AVERAGE_DECAY = 0.99\n",
    "\n",
    "# 模型保存的路径和文件名\n",
    "MODEL_SAVE_PATH = \"model/\"\n",
    "MODEL_NAME = \"model.ckpt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(mnist):\n",
    "    \n",
    "    # 清理图中保存的变量，重置图\n",
    "    tf.reset_default_graph()\n",
    "    # 定义输入输出\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, [None,INPUT_NODE],name=\"x-input\")\n",
    "    y_ = tf.placeholder(tf.float32, [None,OUTPUT_NODE],name=\"y-input\")\n",
    "\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(REGULARAZTION_RATE)\n",
    "    \n",
    "    # 直接使用inference中定义的前向传播过程\n",
    "    y = inference(x, regularizer)\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    \n",
    "    # 定义损失函数、学习率、滑动平均操作以及训练过程\n",
    "    \n",
    "    #print(\"train name:\",tf.get_variable_scope().name)\n",
    "    \n",
    "    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "    \n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "    \n",
    "    loss = cross_entropy_mean + tf.add_n(tf.get_collection('losses'))\n",
    "    \n",
    "    \n",
    "    learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE,global_step,mnist.train.num_examples/BATCH_SIZE,LEARNING_RATE_DECAY,staircase=True)\n",
    "    \n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    with tf.control_dependencies([train_step, variables_averages_op]):\n",
    "        train_op = tf.no_op(name='train')   \n",
    "    \n",
    "    #初始化Tensorflow持久化类\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        \n",
    "        for i in range(TRAINING_STEPS):\n",
    "            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "            _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict={x:xs,y_:ys})\n",
    "            \n",
    "            # 每1000轮保存一次模型\n",
    "            if i % 1000 == 0:\n",
    "                print(loss_value, step)\n",
    "                # 输出在当前batch上的损失\n",
    "                print(\"After \" + str(step) +\" training steps, loss on training batch is \" +str(loss_value)+ \". \")\n",
    "                saver.save(sess, os.path.join(MODEL_SAVE_PATH,MODEL_NAME), global_step = global_step)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)\n",
    "    train(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "layer1/weights:0\n",
      "layer1/biases:0\n",
      "layer2/weights:0\n",
      "layer2/biases:0\n",
      "2.885 1\n",
      "After 1 training steps, loss on training batch is 2.885. \n",
      "0.309594 1001\n",
      "After 1001 training steps, loss on training batch is 0.309594. \n",
      "0.220914 2001\n",
      "After 2001 training steps, loss on training batch is 0.220914. \n",
      "0.151756 3001\n",
      "After 3001 training steps, loss on training batch is 0.151756. \n",
      "0.131714 4001\n",
      "After 4001 training steps, loss on training batch is 0.131714. \n",
      "0.116761 5001\n",
      "After 5001 training steps, loss on training batch is 0.116761. \n",
      "0.093061 6001\n",
      "After 6001 training steps, loss on training batch is 0.093061. \n",
      "0.0888082 7001\n",
      "After 7001 training steps, loss on training batch is 0.0888082. \n",
      "0.0815856 8001\n",
      "After 8001 training steps, loss on training batch is 0.0815856. \n",
      "0.0834794 9001\n",
      "After 9001 training steps, loss on training batch is 0.0834794. \n",
      "0.0673476 10001\n",
      "After 10001 training steps, loss on training batch is 0.0673476. \n",
      "0.0624253 11001\n",
      "After 11001 training steps, loss on training batch is 0.0624253. \n",
      "0.0607762 12001\n",
      "After 12001 training steps, loss on training batch is 0.0607762. \n",
      "0.0584002 13001\n",
      "After 13001 training steps, loss on training batch is 0.0584002. \n",
      "0.0608668 14001\n",
      "After 14001 training steps, loss on training batch is 0.0608668. \n",
      "0.0507428 15001\n",
      "After 15001 training steps, loss on training batch is 0.0507428. \n",
      "0.0470136 16001\n",
      "After 16001 training steps, loss on training batch is 0.0470136. \n",
      "0.0453939 17001\n",
      "After 17001 training steps, loss on training batch is 0.0453939. \n",
      "0.0448128 18001\n",
      "After 18001 training steps, loss on training batch is 0.0448128. \n",
      "0.0448716 19001\n",
      "After 19001 training steps, loss on training batch is 0.0448716. \n",
      "0.0439322 20001\n",
      "After 20001 training steps, loss on training batch is 0.0439322. \n",
      "0.0398295 21001\n",
      "After 21001 training steps, loss on training batch is 0.0398295. \n",
      "0.0441146 22001\n",
      "After 22001 training steps, loss on training batch is 0.0441146. \n",
      "0.0364426 23001\n",
      "After 23001 training steps, loss on training batch is 0.0364426. \n",
      "0.0410183 24001\n",
      "After 24001 training steps, loss on training batch is 0.0410183. \n",
      "0.0402219 25001\n",
      "After 25001 training steps, loss on training batch is 0.0402219. \n",
      "0.0370491 26001\n",
      "After 26001 training steps, loss on training batch is 0.0370491. \n",
      "0.0385104 27001\n",
      "After 27001 training steps, loss on training batch is 0.0385104. \n",
      "0.0361435 28001\n",
      "After 28001 training steps, loss on training batch is 0.0361435. \n",
      "0.0444614 29001\n",
      "After 29001 training steps, loss on training batch is 0.0444614. \n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**注意**：多次运行调试程序的话需要在train()函数前加`tf.reset_default_graph()`以重置图，清理已存在的变量，否则会报错。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
